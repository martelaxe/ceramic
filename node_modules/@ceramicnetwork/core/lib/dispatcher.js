"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.Dispatcher = void 0;
const cids_1 = __importDefault(require("cids"));
const lodash_clonedeep_1 = __importDefault(require("lodash.clonedeep"));
const common_1 = require("@ceramicnetwork/common");
const pubsub_message_1 = require("./pubsub/pubsub-message");
const pubsub_1 = require("./pubsub/pubsub");
const message_bus_1 = require("./pubsub/message-bus");
const lru_map_1 = require("lru_map");
const pubsub_keepalive_1 = require("./pubsub/pubsub-keepalive");
const pubsub_ratelimit_1 = require("./pubsub/pubsub-ratelimit");
const IPFS_GET_RETRIES = 3;
const IPFS_GET_TIMEOUT = 30000;
const IPFS_MAX_COMMIT_SIZE = 256000;
const IPFS_RESUBSCRIBE_INTERVAL_DELAY = 1000 * 15;
const MAX_PUBSUB_PUBLISH_INTERVAL = 60 * 1000;
const IPFS_CACHE_SIZE = 1024;
function messageTypeToString(type) {
    switch (type) {
        case pubsub_message_1.MsgType.UPDATE:
            return 'Update';
        case pubsub_message_1.MsgType.QUERY:
            return 'Query';
        case pubsub_message_1.MsgType.RESPONSE:
            return 'Response';
        case pubsub_message_1.MsgType.KEEPALIVE:
            return 'Keepalive';
        default:
            throw new common_1.UnreachableCaseError(type, `Unsupported message type`);
    }
}
class Dispatcher {
    constructor(_ipfs, topic, repository, _logger, _pubsubLogger, maxQueriesPerSecond) {
        this._ipfs = _ipfs;
        this.topic = topic;
        this.repository = repository;
        this._logger = _logger;
        this._pubsubLogger = _pubsubLogger;
        const pubsub = new pubsub_1.Pubsub(_ipfs, topic, IPFS_RESUBSCRIBE_INTERVAL_DELAY, _pubsubLogger, _logger);
        this.messageBus = new message_bus_1.MessageBus(new pubsub_ratelimit_1.PubsubRateLimit(new pubsub_keepalive_1.PubsubKeepalive(pubsub, MAX_PUBSUB_PUBLISH_INTERVAL), _logger, maxQueriesPerSecond));
        this.messageBus.subscribe(this.handleMessage.bind(this));
        this.dagNodeCache = new lru_map_1.LRUMap(IPFS_CACHE_SIZE);
    }
    async storeCommit(data, streamId) {
        try {
            if (common_1.StreamUtils.isSignedCommitContainer(data)) {
                const { jws, linkedBlock } = data;
                const cid = await this._ipfs.dag.put(jws, { format: 'dag-jose', hashAlg: 'sha2-256' });
                await this._ipfs.block.put(linkedBlock, { cid: jws.link.toString() });
                await this._restrictCommitSize(jws.link.toString());
                await this._restrictCommitSize(cid);
                return cid;
            }
            const cid = await this._ipfs.dag.put(data);
            await this._restrictCommitSize(cid);
            return cid;
        }
        catch (e) {
            if (streamId) {
                this._logger.err(`Error while storing commit to IPFS for stream ${streamId.toString()}: ${e}`);
            }
            else {
                this._logger.err(`Error while storing commit to IPFS: ${e}`);
            }
            throw e;
        }
    }
    async retrieveCommit(cid, streamId) {
        try {
            const result = await this._getFromIpfs(cid);
            await this._restrictCommitSize(cid);
            return result;
        }
        catch (e) {
            if (streamId) {
                this._logger.err(`Error while loading commit CID ${cid.toString()} from IPFS for stream ${streamId.toString()}: ${e}`);
            }
            else {
                this._logger.err(`Error while loading commit CID ${cid.toString()} from IPFS: ${e}`);
            }
            throw e;
        }
    }
    async retrieveFromIPFS(cid, path) {
        try {
            return this._getFromIpfs(cid, path);
        }
        catch (e) {
            this._logger.err(`Error while loading CID ${cid.toString()} from IPFS: ${e}`);
            throw e;
        }
    }
    async _getFromIpfs(cid, path) {
        const asCid = typeof cid === 'string' ? new cids_1.default(cid) : cid;
        const cidAndPath = path ? asCid.toString() + path : asCid.toString();
        const cachedDagNode = await this.dagNodeCache.get(cidAndPath);
        if (cachedDagNode)
            return lodash_clonedeep_1.default(cachedDagNode);
        let dagResult = null;
        for (let retries = IPFS_GET_RETRIES - 1; retries >= 0 && dagResult == null; retries--) {
            try {
                dagResult = await this._ipfs.dag.get(asCid, { timeout: IPFS_GET_TIMEOUT, path });
            }
            catch (err) {
                if (err.code == 'ERR_TIMEOUT' ||
                    err.name == 'TimeoutError' ||
                    err.message == 'Request timed out') {
                    console.warn(`Timeout error while loading CID ${cid.toString()} from IPFS. ${retries} retries remain`);
                    if (retries > 0) {
                        continue;
                    }
                }
                throw err;
            }
        }
        await this.dagNodeCache.set(cidAndPath, dagResult.value);
        return lodash_clonedeep_1.default(dagResult.value);
    }
    async _restrictCommitSize(cid) {
        const stat = await this._ipfs.block.stat(cid, { timeout: IPFS_GET_TIMEOUT });
        if (stat.size > IPFS_MAX_COMMIT_SIZE) {
            throw new Error(`${cid.toString()} commit size ${stat.size} exceeds the maximum block size of ${IPFS_MAX_COMMIT_SIZE}`);
        }
    }
    publishTip(streamId, tip) {
        return this.publish({ typ: pubsub_message_1.MsgType.UPDATE, stream: streamId, tip: tip });
    }
    async handleMessage(message) {
        try {
            switch (message.typ) {
                case pubsub_message_1.MsgType.UPDATE:
                    await this._handleUpdateMessage(message);
                    break;
                case pubsub_message_1.MsgType.QUERY:
                    await this._handleQueryMessage(message);
                    break;
                case pubsub_message_1.MsgType.RESPONSE:
                    await this._handleResponseMessage(message);
                    break;
                case pubsub_message_1.MsgType.KEEPALIVE:
                    break;
                default:
                    throw new common_1.UnreachableCaseError(message, `Unsupported message type`);
            }
        }
        catch (e) {
            this._logger.err(`Error while processing ${messageTypeToString(message.typ)} message from pubsub: ${e}`);
            this._logger.err(e);
        }
    }
    async _handleUpdateMessage(message) {
        const { stream: streamId, tip } = message;
        this.repository.stateManager.update(streamId, tip);
    }
    async _handleQueryMessage(message) {
        const { stream: streamId, id } = message;
        const streamState = await this.repository.streamState(streamId);
        if (streamState) {
            const tip = streamState.log[streamState.log.length - 1].cid;
            const tipMap = new Map().set(streamId.toString(), tip);
            this.publish({ typ: pubsub_message_1.MsgType.RESPONSE, id, tips: tipMap });
        }
    }
    async _handleResponseMessage(message) {
        const { id: queryId, tips } = message;
        const expectedStreamID = this.messageBus.outstandingQueries.get(queryId);
        if (expectedStreamID) {
            const newTip = tips.get(expectedStreamID.toString());
            if (!newTip) {
                throw new Error("Response to query with ID '" +
                    queryId +
                    "' is missing expected new tip for StreamID '" +
                    expectedStreamID +
                    "'");
            }
            this.repository.stateManager.update(expectedStreamID, newTip);
            this.messageBus.outstandingQueries.delete(queryId);
        }
    }
    async close() {
        this.messageBus.unsubscribe();
    }
    publish(message) {
        return this.messageBus.next(message);
    }
}
exports.Dispatcher = Dispatcher;
//# sourceMappingURL=dispatcher.js.map